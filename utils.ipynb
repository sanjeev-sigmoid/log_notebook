{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0558200f-2051-4dca-8eb6-98104d466799",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logging Code\n",
    "import os\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Log Data\").getOrCreate()\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        # Initialize instance variables\n",
    "        self.server = os.getenv(\"server\")  # safer than os.environ[]\n",
    "        self.database = os.getenv(\"database\")\n",
    "        self.user = os.getenv(\"user\")\n",
    "        self.password = os.getenv(\"password\")\n",
    "        self.port = os.getenv(\"port\")\n",
    "\n",
    "    def _get_widgets_parameters(self):\n",
    "        \"\"\"\n",
    "        Set parameters from widgets with default values  \n",
    "        \"\"\"\n",
    "\n",
    "        widget_defaults = {\n",
    "            'datafactory_name': 'default_datafactory_name',\n",
    "            'pipeline_name': 'default_pipeline_name',\n",
    "            'pipelinerunid': 'default_pipelinerunid',\n",
    "            'trigger_type': 'default_trigger_type',\n",
    "            'trigger_id': 'default_trigger_id',\n",
    "            'trigger_name': 'default_trigger_name',\n",
    "            'trigger_time': '2000-01-01T00:00:00.00Z'\n",
    "        }\n",
    "\n",
    "        # Get data in base parameters from azure data factory and if not passed set default values\n",
    "        for widget, default_value in widget_defaults.items():\n",
    "            try:\n",
    "                setattr(self, widget, dbutils.widgets.get(widget))\n",
    "            except:\n",
    "                setattr(self, widget, default_value)\n",
    "\n",
    "        # Convert utc time to timestamp format\n",
    "        self.trigger_time = datetime.datetime.strptime(self.trigger_time.replace('Z', '')[:-1], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"\n",
    "        Connect to the SQL Server database\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Construct the JDBC URL with connection parameters\n",
    "            self.jdbc_url = f\"jdbc:sqlserver://{self.server}:{self.port};database={self.database};user={self.user};password={self.password};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "\n",
    "            # Fetch the driver manager from your spark context\n",
    "            self.driver_manager = spark._sc._gateway.jvm.java.sql.DriverManager\n",
    "\n",
    "            # Create a connection object using a jdbc-url\n",
    "            self.connection = self.driver_manager.getConnection(self.jdbc_url)\n",
    "\n",
    "            print('Connection Successful')\n",
    "\n",
    "            # Call _get_parameters_from_widgets() after connection is established\n",
    "            self._get_widgets_parameters()  \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e.java_exception.getMessage()}\")\n",
    "\n",
    "    def log_pipeline(self, user_friendly_name, correlation_id, status):\n",
    "        \"\"\"\n",
    "        Logs the pipeline execution details in the SQL Server database using log_pipeline stored procedure.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Write your SQL statement as a string\n",
    "            sql = \"EXEC log_pipeline @user_friendly_name=?, @datafactory_name=?, @pipeline_name=?, @pipelinerunid=?, \" \\\n",
    "                \"@correlation_id=?, @duration=?, @trigger_type=?, @trigger_id=?, @trigger_name=?, @trigger_time=?,\" \\\n",
    "                \"@start_time=?, @end_time=?, @status=?;\"\n",
    "            \n",
    "            if status == \"In Progress\":\n",
    "                self.start_time1 = datetime.datetime.now()\n",
    "                start_time1 = self.start_time1\n",
    "                end_time1 = None\n",
    "                duration1 = None\n",
    "            elif status == \"Success\":\n",
    "                end_time1 = datetime.datetime.now()\n",
    "                duration_diff1 = end_time1 - self.start_time1\n",
    "                duration1 = str(duration_diff1)\n",
    "                start_time1 = self.start_time1\n",
    "\n",
    "            # Create callable statement and execute it\n",
    "            self.statement = self.connection.prepareStatement(sql)\n",
    "\n",
    "            # Set the values of the parameters\n",
    "            self.statement.setString(1, user_friendly_name)\n",
    "            self.statement.setString(2, self.datafactory_name)\n",
    "            self.statement.setString(3, self.pipeline_name)\n",
    "            self.statement.setString(4, self.pipelinerunid)\n",
    "            self.statement.setString(5, correlation_id)\n",
    "            self.statement.setString(6, duration1)\n",
    "            self.statement.setString(7, self.trigger_type)\n",
    "            self.statement.setString(8, self.trigger_id)\n",
    "            self.statement.setString(9, self.trigger_name)\n",
    "            self.statement.setTimestamp(10, self.trigger_time)\n",
    "            self.statement.setTimestamp(11, start_time1)\n",
    "            self.statement.setTimestamp(12, end_time1)\n",
    "            self.statement.setString(13, status)\n",
    "            \n",
    "            # Execute the stored procedure\n",
    "            self.statement.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "    def log_pipeline_activity(self, user_friendly_name, cluster_name, activity_runid, \n",
    "                              activity_name, source_type, sink_type, status):\n",
    "        \"\"\"\n",
    "        Logs the pipeline execution details in the SQL Server database using log_pipeline_activity stored procedure.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Write your SQL statement as a string\n",
    "            sql = \"EXEC log_pipeline_activity @user_friendly_name=?, @cluster_name=?, @duration=?, @activity_runid=?,\" \\\n",
    "                \"@pipeline_runid=?, @pipeline_name=?, @activity_name=?, @start_time=?, @end_time=?, @source_type=?,\" \\\n",
    "                \"@sink_type=?, @status=?;\"\n",
    "\n",
    "            # Create callable statement and execute it\n",
    "            self.statement = self.connection.prepareStatement(sql)\n",
    "\n",
    "            if status == \"In Progress\":\n",
    "                self.start_time2 = datetime.datetime.now()\n",
    "                start_time2 = self.start_time2\n",
    "                end_time2 = None\n",
    "                duration2 = None\n",
    "            elif status == \"Success\":\n",
    "                end_time2 = datetime.datetime.now()\n",
    "                duration_diff2 = end_time2 - self.start_time2\n",
    "                duration2 = str(duration_diff2)\n",
    "                start_time2 = self.start_time2\n",
    "                \n",
    "            # Set the values of the parameters\n",
    "            self.statement.setString(1, user_friendly_name)\n",
    "            self.statement.setString(2, cluster_name)\n",
    "            self.statement.setString(3, duration2) \n",
    "            self.statement.setString(4, activity_runid)\n",
    "            self.statement.setString(5, self.pipelinerunid)\n",
    "            self.statement.setString(6, self.pipeline_name)\n",
    "            self.statement.setString(7, activity_name)\n",
    "            self.statement.setTimestamp(8, start_time2)  \n",
    "            self.statement.setTimestamp(9, end_time2)  \n",
    "            self.statement.setString(10, source_type)\n",
    "            self.statement.setString(11, sink_type)\n",
    "            self.statement.setString(12, status)\n",
    "\n",
    "            # Execute the stored procedure\n",
    "            self.statement.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"\n",
    "        Close the connection to the SQL Server database.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the connection attribute exists and if it is not None\n",
    "        if hasattr(self, 'connection') and self.connection is not None:\n",
    "            self.connection.close()\n",
    "            self.connection = None\n",
    "            print('Connection Closed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5e408e-4c68-4cf1-8fc5-6bcccd79de23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What to execute to log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72079f0-8470-4f13-aff9-154af9e2841f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Successful\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of Logger and connect to database\n",
    "# logger = Logger()\n",
    "# logger.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a39f8c-3c66-4293-b17f-73849d45e2ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log using log_pipeline stored procedure \n",
    "# logger.log_pipeline(\n",
    "#     user_friendly_name=\"Uttam\",\n",
    "#     correlation_id=\"abc123\",\n",
    "#     status=\"In Progress\",\n",
    "# )\n",
    "\n",
    "##-- Notebook codes --##\n",
    "\n",
    "# time.sleep(2)\n",
    "# logger.log_pipeline(\n",
    "#     user_friendly_name=\"Uttam\",\n",
    "#     correlation_id=\"abc123\",\n",
    "#     status=\"Success\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530ff1f1-c57d-4080-9ca7-76f48aa48c0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.026049\n"
     ]
    }
   ],
   "source": [
    "# Log using log_pipeline_activity stored procedure \n",
    "# logger.log_pipeline_activity(\n",
    "#     user_friendly_name=\"Uttam\",\n",
    "#     cluster_name=\"MyCluster\",\n",
    "#     activity_runid=\"activity123\",\n",
    "#     activity_name=\"MyActivity\",\n",
    "#     source_type=\"Source\",\n",
    "#     sink_type=\"Sink\",\n",
    "#     status=\"In Progress\"\n",
    "# )\n",
    "\n",
    "##-- Any activity --##\n",
    "\n",
    "# logger.log_pipeline_activity(\n",
    "#     user_friendly_name=\"Uttam\",\n",
    "#     cluster_name=\"MyCluster\",\n",
    "#     activity_runid=\"activity123\",\n",
    "#     activity_name=\"MyActivity\",\n",
    "#     source_type=\"Source\",\n",
    "#     sink_type=\"Sink\",\n",
    "#     status=\"Success\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57973bd6-df2a-4c9b-9ab1-94328d9c7875",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Closed\n"
     ]
    }
   ],
   "source": [
    "# logger.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "299c9cba-0df1-4b6a-a2a5-4c5fb1826673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
